{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUE Tracker LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83fb25a05150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumptojsonfile( filename, data, indent=2):\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(json.dumps(data, indent=indent))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(db):\n",
    "    ids = db['_all_docs']\n",
    "    ids = [x[\"id\"] for x in ids[\"rows\"] if x[\"id\"][0] != \"_\" ]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2s(v):\n",
    "    if (v == {}):\n",
    "        return \"\"\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank( d ):\n",
    "    if (d == None):\n",
    "        return ''\n",
    "    else:\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2sblank(doc, key):\n",
    "    return blank(d2s(doc.get(key,\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_list(v):\n",
    "    if (v.__class__ == [].__class__):\n",
    "        return v\n",
    "    return [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dict( dicts ):\n",
    "    return dict((v,k) for k, v in dicts.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-987905de5c1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#stopwordp = re.compile(\"^[a-zA-Z0-9\\\\._\\\\/\\\\\\\\]+$\",re.I)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mstopwordp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^[a-zA-Z]+$\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "_stopwords = False\n",
    "def stopwords():\n",
    "    global _stopwords\n",
    "    if (_stopwords == False):\n",
    "        try:\n",
    "            _stopwords = set(open(\"stop_words\").read().splitlines())\n",
    "        except:\n",
    "            _stopwords = dict()\n",
    "        _stopwords.add(\"\")\n",
    "        _stopwords.add(\"'s\")\n",
    "    return _stopwords\n",
    "\n",
    "#stopwordp = re.compile(\"^[a-zA-Z0-9\\\\._\\\\/\\\\\\\\]+$\",re.I)\n",
    "stopwordp = re.compile(\"^[a-zA-Z]+$\",re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords( tokens ):\n",
    "    global stopwordp\n",
    "    sw = stopwords()\n",
    "    w1 = [x for x in tokens if not(x in sw)] \n",
    "    return [y for y in w1 if not(None == stopwordp.match(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1906d60d6f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#_tokenizer = TreebankWordTokenizer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_stopwords\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def tokenize( text, tokenizer=_tokenizer):\n",
    "    tokens = filter_stopwords( tokenizer.tokenize( text.lower() ) )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words_by_frequency(dicts, doc_db, filter_fun):\n",
    "    newdict = dict()\n",
    "    newdocs = dict()\n",
    "    ndocs = dict()\n",
    "    removedwords = set()\n",
    "    # count words\n",
    "    for key, doc in doc_db.iteritems():\n",
    "        for w in doc:\n",
    "            ndocs[w] = ndocs.get(w,0) + 1\n",
    "    keptwords = dict((x,c) for x,c in ndocs.iteritems() if filter_fun(x,c))\n",
    "    newmap = dict()\n",
    "    cnt = 2\n",
    "    for word, count in keptwords:\n",
    "        newmap\n",
    "    # drop the bad words\n",
    "    for key, doc in doc_db.iteritems():\n",
    "        ndocs[key] = [w for w in doc if w in keptwords]\n",
    "    # map the words to new values\n",
    "    for key, doc in doc_db.iteritems():\n",
    "        ndocs[key] = [w for w in doc if w in keptwords]\n",
    "    raise Exception(\"Not Done Don't Use\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words_in_only_n_file(dicts, doc_db, n=1):\n",
    "    ndocs = len(doc_db)\n",
    "    mincount = n\n",
    "    def thresh(word, count):\n",
    "        return (count >= mincount)\n",
    "    \n",
    "    return filter_words_by_frequency( dicts, doc_db, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uncommon_word(dicts, doc_db, threshold=0.01):\n",
    "    ndocs = len(doc_db)\n",
    "    mincount = max(1,math.ceil(ndocs * threshold))\n",
    "    def thresh(word, count):\n",
    "        return (count >= mincount)    \n",
    "    return filter_words_by_frequency( dicts, doc_db, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_common_word(dicts, doc_db, threshold=0.8):\n",
    "    ndocs = len(doc_db)\n",
    "    mincount = max(1,math.ceil(ndocs * threshold))\n",
    "    def thresh(word, count):\n",
    "        return (count <= mincount)    \n",
    "    return filter_words_by_frequency( dicts, doc_db, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_uncommon_common_word(dicts, doc_db, lowthreshold=0.01, highthreshold=0.8):\n",
    "    ndocs = len(doc_db)\n",
    "    mincount = max(1,math.ceil(ndocs * lowthreshold))\n",
    "    maxcount = max(1,math.ceil(ndocs * highthreshold))\n",
    "    def thresh(word, count):\n",
    "        return (count >= mincount and count <= maxcount)\n",
    "    return filter_words_by_frequency( dicts, doc_db, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e649e8ae3769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# count tokens and make dictionary from them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def add_doc( doc, dicts, counter, tokenizer=_tokenizer ):\n",
    "    tokens = tokenize( doc )\n",
    "    # count tokens and make dictionary from them\n",
    "    counts = dict()\n",
    "    for token in tokens:\n",
    "        if (dicts.get(token, -1) == -1):\n",
    "            dicts[token] = counter\n",
    "            counter += 1\n",
    "        id = dicts[token]\n",
    "        oldcount = counts.get(id, 0)\n",
    "        counts[id] = oldcount + 1\n",
    "    return (counter, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_document( doc ):\n",
    "    doc_comments = as_list(doc.get(\"comments\",[]))\n",
    "    comments = \"\\n\".join([ d2s(x.get(\"what\",\"\")) + d2s(x.get(\"content\",\"\"))  or '' for x in doc_comments])\n",
    "    outdoc = \"\\n\".join([\n",
    "            d2sblank(doc,\"title\"),\n",
    "            d2sblank(doc,\"description\"),\n",
    "            d2sblank(doc,\"content\"),\n",
    "            blank(comments)\n",
    "            ])\n",
    "    return outdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-19-4d8474e1248a>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-4d8474e1248a>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    if (len(counts) == 0):\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "def load_lda_docs(db, ids, extractor=extract_text_from_document, tokenizer=_tokenizer ):\n",
    "    dicts  = {\"_EMPTY_\":1}\n",
    "    counter = 2\n",
    "    docs = dict()\n",
    "    for id in ids:\n",
    "        print(id)\n",
    "        doc = db[id]\n",
    "        #if (id == 180):\n",
    "        #    pdb.set_trace()\n",
    "        #    print(doc[\"content\"])\n",
    "        outdoc = extractor( doc )\n",
    "        counter, counts = add_doc( outdoc, dicts, counter, tokenizer=tokenizer )\n",
    "\tif (len(counts) == 0):\n",
    "            counts = {1:1}\n",
    "        docs[id] = counts\n",
    "    return docs, dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vr_lda( doc ):\n",
    "    return \"| \" + \" \".join([ str(key) + \":\" + str(doc[key]) for key in doc ]) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vr_lda_input( docs, dicts, filename = \"out/vr_lda_input.lda.txt\", filenameid = \"out/vr_lda_input.ids.txt\"):\n",
    "    file = open( filename, \"w+\")\n",
    "    docids = docs.keys()\n",
    "    docids.sort()\n",
    "    for docid in docids:\n",
    "        file.write( doc_to_vr_lda( docs[docid] ) )\n",
    "        file.write\n",
    "    file.close()\n",
    "    \n",
    "    ifile = open( filenameid, \"w\" )\n",
    "    ifile.write( json.dumps([str(docid) for docid in docids], indent=2))\n",
    "    ifile.close()\n",
    "    # words\n",
    "    filenamewords = \"out/vr_lda_input.words.txt\"\n",
    "    wfile = open( filenamewords, \"w\" )\n",
    "    wfile.write(json.dumps(dicts, indent=2))\n",
    "    wfile.close()\n",
    "\n",
    "    return filename, filenameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_bits( dicts ):\n",
    "    return int(math.ceil(math.log(len(dicts),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_lda_command( filename, topics, dicts, alpha=0.01, beta=0.01, passes=1):\n",
    "    stopics = str(topics)\n",
    "    bits = dict_bits(dicts)\n",
    "    # removed cache file\n",
    "    try:\n",
    "        os.remove(\"out/topic-%s.dat.cache\" % (stopics))\n",
    "    except:\n",
    "        True\n",
    "    #return \" %s --lda %s --lda_alpha 0.1 --lda_rho 0.1 --minibatch 256 --power_t 0.5 --initial_t 1 -b %d --passes 2 -c  -p out/predictions-%s.dat --readable_model out/topics-%s.dat %s\" % (\n",
    "    return \" %s --lda %s --lda_alpha %s --lda_rho %s --minibatch 256 --power_t 0.5 --initial_t 1 -b %d -c --passes %d -p out/predictions-%s.dat --readable_model out/topics-%s.dat  %s\" % (\n",
    "        \"vw\",\n",
    "        stopics,\n",
    "        alpha,\n",
    "        beta,\n",
    "        bits,\n",
    "        passes,\n",
    "        stopics,\n",
    "        stopics,\n",
    "        filename\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_topics( n, dicts, readable_model_lines, max_words = 256 ):\n",
    "    '''  this returns a summary of the topic model as words and a matrix of terms '''\n",
    "    nlines = len( readable_model_lines )\n",
    "    topics = [[0 for x in range(0, nlines)] for y in range(0, n)] \n",
    "    word = 0\n",
    "    # we need version 7 now\n",
    "    if (\"Version\" in readable_model_lines[0]):\n",
    "        readable_model_lines.pop(0)\n",
    "    for line in readable_model_lines:\n",
    "        if (not (\":\" in line) and not (\"Version\" in line)):            \n",
    "            line = line.rstrip()\n",
    "            topic = 0\n",
    "            #print(\"[\"+line+\"]\")\n",
    "            elms = [float(x) for x in line.split(\" \")[1:]]\n",
    "            for topic in range(0, n):\n",
    "                topics[topic][word] = elms[topic]\n",
    "            word += 1\n",
    "    # now we have that matrix\n",
    "    # per each topic find the most prevelant word\n",
    "    summary = [[] for x in range(0, n)]\n",
    "    revdict = reverse_dict( dicts )\n",
    "    for topici in range(0, n):\n",
    "        topic = topics[topici]\n",
    "        #print(\"Topic Length %d\" % (len(topic)))\n",
    "        #print(\"RevDict Length %d\" % (len(revdict)))\n",
    "        #print(\"Dicts Length %d\" % (len(dicts)))\n",
    "        indices = range(0,len(topic))\n",
    "        indices.sort( key = lambda i: topic[i], reverse = True )\n",
    "        words = [ revdict.get(i,(\"NOTFOUND: %d\" % i)) for i in indices[0:max_words] ]\n",
    "        summary[ topici ] = words\n",
    "    return topics , summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_indices( x ):\n",
    "    indices = range(0,len(x))\n",
    "    indices.sort( key = lambda i: x[i] )\n",
    "    return indices;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_topics_from_file(n, dicts, readable_model_filename ):\n",
    "    file = open( readable_model_filename, \"r\" )\n",
    "    text = file.readlines()\n",
    "    file.close()\n",
    "    return summarize_topics(n, dicts, text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document_topic_matrix(n, lines, passes=1):\n",
    "    ''' each line is a set of numbers indicating the topic association '''\n",
    "    if (passes > 1):\n",
    "        lines = lines[len(lines) - len(lines)/passes:]\n",
    "    nlines = len( lines )\n",
    "    docs = [[0 for x in range(0, n)] for y in range(0, nlines)] \n",
    "    return [[float(x) for x in line.rstrip().split(\" \")] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document_topic_matrix_from_file( n, document_topic_matrix_filename, passes=1 ):\n",
    "    file = open( document_topic_matrix_filename, \"r\" )\n",
    "    text = file.readlines()\n",
    "    file.close()\n",
    "    return summarize_document_topic_matrix( n, text, passes=passes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Object -- manages most state internally\n",
    "class LDA(object):\n",
    "    def __init__(self, params=None):\n",
    "        global _tokenizer\n",
    "        if (params == None):\n",
    "            params = dict()\n",
    "        self.alpha = params.get(\"alpha\",0.1)\n",
    "        self.beta = params.get(\"beta\",0.1)\n",
    "        self.passes = params.get(\"passes\",1)\n",
    "        self.ntopics = params.get(\"ntopics\", params.get(\"topics\", 20))\n",
    "        self.tokenizer = params.get(\"tokenizer\",_tokenizer)\n",
    "        self.extractor = params.get(\"extractor\", extract_text_from_document)\n",
    "        self.init_params = params\n",
    "        self.documents_loaded = False\n",
    "        self.lda_prepared = False\n",
    "        self.command = None\n",
    "        self.lda_has_run = False\n",
    "\n",
    "    def load_documents(self, document_db, ids):\n",
    "        docs, dicts = load_lda_docs(document_db, ids, extractor=self.extractor, tokenizer=self.tokenizer)\n",
    "        self.ids = ids\n",
    "        self.docs = docs\n",
    "        self.dicts = dicts\n",
    "        self.words = {v:k for k, v in dicts.items()}\n",
    "        self.documents_loaded = True\n",
    "        return (self.docs, self.dicts, self.words)\n",
    "\n",
    "    def prepare_lda(self):\n",
    "        if (not self.documents_loaded):\n",
    "            raise Exception(\"Documents are not loaded!\")\n",
    "        filename, _ = make_vr_lda_input( self.docs, self.dicts )\n",
    "        self.filename = filename\n",
    "        self.lda_prepared = True\n",
    "        \n",
    "    def lda_command(self):\n",
    "        if (not self.lda_prepared):\n",
    "            raise Exception(\"LDA Not prepared\")\n",
    "        command = vm_lda_command(self.filename, self.ntopics, self.dicts, alpha=self.alpha, beta=self.beta, passes=self.passes)\n",
    "        self.command = command\n",
    "        print(\"Command %s\" % command)\n",
    "        return command\n",
    "\n",
    "    def run_lda(self):\n",
    "        if (self.command == None):\n",
    "            self.lda_command()\n",
    "        print(self.command)\n",
    "        os.system( self.command )\n",
    "        self.lda_has_run = True\n",
    "        return self.lda_has_run\n",
    "\n",
    "    def summarize_topics(self):\n",
    "        if (not self.lda_has_run):\n",
    "            raise Exception(\"LDA Not Run -- Run LDA first\")\n",
    "        self.topic_file_name = (\"out/topics-%s.dat\" % self.ntopics)\n",
    "        topics, summary = summarize_topics_from_file( self.ntopics, self.dicts, self.topic_file_name  )\n",
    "        self.topics = topics\n",
    "        self.summary = summary\n",
    "        return (topics, summary)\n",
    "\n",
    "    def summarize_document_topic_matrix(self):\n",
    "        self.prediction_file = (\"out/predictions-%s.dat\" % self.ntopics)\n",
    "        document_topic_matrix = summarize_document_topic_matrix_from_file( self.ntopics, self.prediction_file , passes=self.passes )\n",
    "        doc_top_mat_map = dict( (self.ids[i], document_topic_matrix[i]) for i in range(0,len(self.ids)) )\n",
    "        self.document_topic_matrix = document_topic_matrix\n",
    "        self.doc_top_mat_map = doc_top_mat_map\n",
    "        return (document_topic_matrix, doc_top_mat_map)\n",
    "\n",
    "    def dump_doc_top_to_csv(self):\n",
    "        try:\n",
    "            ids = []\n",
    "            try:\n",
    "                ids = [ int(x) for x in self.doc_top_mat_map.keys() ]\n",
    "                ids.sort()\n",
    "                #ids = [str(x) for x in ids]\n",
    "            except:\n",
    "                ids = [x for x in self.doc_top_mat_map.keys()]\n",
    "                ids.sort()\n",
    "            doctop = self.doc_top_mat_map\n",
    "            head = [\"Doc\"] + [\"T%s\" % x for x in range(1,self.ntopics+1)]\n",
    "            with open('out/document_topic_map.csv', 'wb') as f:\n",
    "                writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(head)\n",
    "                for id in ids:\n",
    "                    writer.writerow([id]+doctop[id])\n",
    "            with open('out/document_topic_map_norm.csv', 'wb') as f:\n",
    "                writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(head)\n",
    "                for id in ids:\n",
    "                    s = sum(doctop[id])\n",
    "                    writer.writerow([id]+[x/s for x in doctop[id]])\n",
    "        except AttributeError:\n",
    "            raise Exception(\"Please run summarize_topics and summarize_document_topic_matrix\")\n",
    "                \n",
    "    def dump_to_json(self):\n",
    "        try:\n",
    "            dumptojsonfile(\"out/lda-topics.json\", self.topics)\n",
    "            dumptojsonfile(\"out/summary.json\", self.summary)\n",
    "            dumptojsonfile(\"out/document_topic_matrix.json\", self.document_topic_matrix)\n",
    "            dumptojsonfile(\"out/document_topic_map.json\", self.doc_top_mat_map)\n",
    "            self.dump_doc_top_to_csv()\n",
    "        except AttributeError:\n",
    "            raise Exception(\"Please run summarize_topics and summarize_document_topic_matrix\")\n",
    "        \n",
    "    def run(self):\n",
    "        if (not self.documents_loaded):\n",
    "            raise Exception(\"Documents are not loaded!\")\n",
    "        self.prepare_lda()\n",
    "        self.lda_command()\n",
    "        self.run_lda()\n",
    "        self.summarize_topics()\n",
    "        self.summarize_document_topic_matrix()\n",
    "        self.dump_to_json()\n",
    "        return True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
